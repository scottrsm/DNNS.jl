<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>DNNS.jl Documentation · DNNS</title><meta name="title" content="DNNS.jl Documentation · DNNS"/><meta property="og:title" content="DNNS.jl Documentation · DNNS"/><meta property="twitter:title" content="DNNS.jl Documentation · DNNS"/><meta name="description" content="Documentation for DNNS."/><meta property="og:description" content="Documentation for DNNS."/><meta property="twitter:description" content="Documentation for DNNS."/><script data-outdated-warner src="assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="search_index.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href>DNNS</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li class="is-active"><a class="tocitem" href>DNNS.jl Documentation</a><ul class="internal"><li class="toplevel"><a class="tocitem" href="#Overview"><span>Overview</span></a></li><li><a class="tocitem" href="#Data-Structures"><span>Data Structures</span></a></li><li><a class="tocitem" href="#Non-linear-Activation-Functions"><span>Non-linear Activation Functions</span></a></li><li><a class="tocitem" href="#Functions"><span>Functions</span></a></li><li><a class="tocitem" href="#Index"><span>Index</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>DNNS.jl Documentation</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>DNNS.jl Documentation</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/scottrsm/DNNS.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/scottrsm/DNNS.jl/blob/main/docs/src/index.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="DNNS.jl-Documentation"><a class="docs-heading-anchor" href="#DNNS.jl-Documentation">DNNS.jl Documentation</a><a id="DNNS.jl-Documentation-1"></a><a class="docs-heading-anchor-permalink" href="#DNNS.jl-Documentation" title="Permalink"></a></h1><h1 id="Overview"><a class="docs-heading-anchor" href="#Overview">Overview</a><a id="Overview-1"></a><a class="docs-heading-anchor-permalink" href="#Overview" title="Permalink"></a></h1><p>Below is a collection of structures and functions which are useful to create (D)eep (N)eural (N)etworks. One of the features of this package is the ability to do <em>Automatic Differentiation</em>. This is accomplished via the structure, <code>AD</code>. The basic arithmatic functions as well as the power, abs, log, exponential, and all of the trigonometric functions from the base  package are overloaded by this package to work with <code>AD</code> &quot;numbers&quot;.</p><p>Additionally, this package adds a notion of a piece-wise linear function via the struct, <code>PWL</code>. This structure, as well as are the structures <code>DLayer</code> and <code>DNN</code>, has been instrumented so  that it can also serve as a function.</p><ul><li>Structures:<ul><li><code>AD:</code> A structure used to implement <code>Automatic Differentiation</code>.</li><li><code>PWL:</code> A structure representing a piece-wise linear function.</li><li><code>DLayer:</code> A structure representing a single layer of a DNN.</li><li><code>DNN:</code> A structure representing a Deep Neural Net which consists of        Some number of <code>DLayers</code>.</li></ul></li><li>Functions<ul><li><code>L1</code></li><li>Non-linear Activation Functions<ul><li><code>sigmoid1</code></li><li><code>sigmoid2</code></li><li><code>sigmoid3</code></li><li><code>softmax</code></li><li><code>relu</code></li><li><code>relur</code></li></ul></li><li>Associated Learning Functions<ul><li><code>loss</code></li><li><code>fit</code></li></ul></li></ul></li></ul><h2 id="Data-Structures"><a class="docs-heading-anchor" href="#Data-Structures">Data Structures</a><a id="Data-Structures-1"></a><a class="docs-heading-anchor-permalink" href="#Data-Structures" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DNNS.AutoDiff.AD" href="#DNNS.AutoDiff.AD"><code>DNNS.AutoDiff.AD</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AD{T}</code></pre><p>Automatic differentiation structure.</p><p>Fields</p><ul><li>v :: T – The value of this structure.</li><li>d :: T – The derivative at this value.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/scottrsm/DNNS.jl/blob/06f29b15d3498bdac29fdcc614246b5f3ebbfb94/src/AutoDiff.jl#L15-L24">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DNNS.PWLF.PWL" href="#DNNS.PWLF.PWL"><code>DNNS.PWLF.PWL</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">PWL{T}</code></pre><p>A structure representing a piece-wise linear function on the Real line.</p><p>In practice, one uses one of two outer constructors to create a <code>PWL</code> struct.</p><p><strong>Type Constraints</strong></p><ul><li><code>T &lt;: Number</code></li><li>The type <code>T</code> must have a total ordering.</li></ul><p><strong>Fields</strong></p><ul><li><code>xs :: Vector{T}</code>  – The &quot;x&quot; values.</li><li><code>ys :: Vector{T}</code>  – The &quot;y&quot; values.</li><li><code>ds :: Vector{T}</code>  – The &quot;slopes&quot; of the segments. Including the left most and right most slopes.</li><li><code>n  :: Int</code>        – The number of &quot;x,y&quot; values.</li></ul><p><strong>Public Constructors</strong></p><p><code>PWL(xs::Vector{T}, y::T, ds::Vector{T})</code> </p><ul><li><code>xs</code> – The <code>x</code> coordinates in ascending order – no duplicates.</li><li><code>y</code>  – The value of <code>y</code> corresponding to the first entry in <code>xs</code>.</li><li><code>ds</code> – The slopes of all &quot;x&quot; intervals as well as the &quot;left&quot; slope of the first         point and the &quot;right&quot; slope of the last point.</li></ul><p><code>PWL(xs::Vector{T}, ys::Vector{T}, ds::Vector{T})</code></p><ul><li><code>xs</code> – The <code>x</code> coordinates in ascending order – no duplicates.</li><li><code>ys</code> – The <code>y</code> coordinates corresponding to each <code>x</code> value.</li><li><code>ds</code> – A 2-Vector consisting of the &quot;left&quot; slope of the first point and the &quot;right&quot;         slope of the last point.</li></ul><p><strong>Examples</strong></p><pre><code class="language-jdoctest hljs">julia&gt; # Create the same (in behavior) Piecewise linear functions in two ways:
julia&gt; pw1 = PWL([1.0, 2.0, 3.0], [2.0, 3.0, 4.0], [0.0, 5.0])

julia&gt; pw2 = PWL([1.0, 2.0, 3.0], 2.0, [0.0, 1.0, 1.0, 5.0])

julia&gt; pw1(2.5)
3.5

julia&gt; pw2(2.5)
3.5</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/scottrsm/DNNS.jl/blob/06f29b15d3498bdac29fdcc614246b5f3ebbfb94/src/PWLF.jl#L10-L54">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DNNS.DLayer" href="#DNNS.DLayer"><code>DNNS.DLayer</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">DLayer{T&lt;:Number}</code></pre><p>A structure representing one layer of a neural net. </p><p><strong>Type Constraints</strong></p><ul><li><code>T &lt;: Number</code></li><li>The type <code>T</code> must have a total ordering.</li></ul><p><strong>Fields</strong></p><ul><li><code>M    :: Matrix{AD{T}}</code>    – The &quot;x&quot; values.</li><li><code>b    :: Vector{AD{T}}</code>    – The &quot;y&quot; values.</li><li><code>op   :: Function</code>         – The &quot;slopes&quot; of the segments.</li><li><code>dims :: Tuple{Int, Int}</code>  – The number of &quot;x,y&quot; values.</li></ul><p><strong>Public Constructors</strong></p><p><code>DLayer(Mn::Matrix{T}, bn::Vector{T}, opn::Function)</code></p><ul><li><code>Mn</code> – A <code>MxN</code>weight matrix.</li><li><code>bn</code> – A <code>N</code>dimensional bias vector.</li><li><code>op</code> – The non-linear threshold function.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/scottrsm/DNNS.jl/blob/06f29b15d3498bdac29fdcc614246b5f3ebbfb94/src/DNNS.jl#L20-L41">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DNNS.DNN" href="#DNNS.DNN"><code>DNNS.DNN</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">DNN{T&lt;:Number}</code></pre><p>A structure representing one layer of a neural net. </p><p><strong>Type Constraints</strong></p><ul><li><code>T &lt;: Number</code></li><li>The type <code>T</code> must have a total ordering.</li></ul><p><strong>Fields</strong></p><ul><li><code>layers :: Vector{DLayer{T}</code> – The neural net layers.</li></ul><p><strong>Public Constructors</strong></p><p><code>DNN(ls::Vector{DLayer{T}}</code></p><ul><li><code>ls</code> – A vector of DLayer.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/scottrsm/DNNS.jl/blob/06f29b15d3498bdac29fdcc614246b5f3ebbfb94/src/DNNS.jl#L85-L101">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DNNS.PWLF.PWL-Tuple{T} where T&lt;:Number" href="#DNNS.PWLF.PWL-Tuple{T} where T&lt;:Number"><code>DNNS.PWLF.PWL</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">(PWL{T})(x::T) where {T&lt;:Number}</code></pre><p>Uses the structure <code>PWL</code> as a piece-wise linear function. </p><p><strong>Type Constraints</strong></p><ul><li><code>T &lt;: Number</code></li></ul><p><strong>Arguments</strong></p><ul><li><code>x :: T</code>  – An input value.</li></ul><p><strong>Return</strong></p><p><code>:: T</code></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/scottrsm/DNNS.jl/blob/06f29b15d3498bdac29fdcc614246b5f3ebbfb94/src/PWLF.jl#L149-L162">source</a></section><section><div><pre><code class="language-julia hljs">(PWL{T})(x::AD{T}) where {T&lt;:Number}</code></pre><p>Uses the structure <code>PWL</code> as a piece-wise linear function. </p><p><strong>Type Constraints</strong></p><ul><li><code>T &lt;: Number</code></li></ul><p><strong>Arguments</strong></p><ul><li><code>x :: AD{T}</code>  – An AutoDiff value.</li></ul><p><strong>Return</strong></p><p><code>:: AD{T}</code></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/scottrsm/DNNS.jl/blob/06f29b15d3498bdac29fdcc614246b5f3ebbfb94/src/PWLF.jl#L175-L188">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DNNS.DLayer-Union{Tuple{DNNS.AutoDiff.AD{T}}, Tuple{T}} where T&lt;:Number" href="#DNNS.DLayer-Union{Tuple{DNNS.AutoDiff.AD{T}}, Tuple{T}} where T&lt;:Number"><code>DNNS.DLayer</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">(L::DLayer{T})(x::AD{T}) where {T &lt;: Number}</code></pre><p>If <code>(M,N) = L.dims</code>, then we may treat the structure, <code>DLayer</code>, as a function: <span>${\cal R}^m \mapsto {\cal R}^n$</span> .</p><p>Takes input <code>x</code> and passes it through the layer.</p><p><strong>Type Constraints</strong></p><ul><li><code>T &lt;: Number</code></li></ul><p><strong>Arguments</strong></p><ul><li><code>x :: AD{T}</code>  – An input value of dimension <code>N</code>.</li></ul><p><strong>Return</strong></p><p><code>::Vector{AD{T}}</code> of dimension <code>M</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/scottrsm/DNNS.jl/blob/06f29b15d3498bdac29fdcc614246b5f3ebbfb94/src/DNNS.jl#L61-L76">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DNNS.DNN-Union{Tuple{AbstractVector{T}}, Tuple{T}} where T&lt;:Number" href="#DNNS.DNN-Union{Tuple{AbstractVector{T}}, Tuple{T}} where T&lt;:Number"><code>DNNS.DNN</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">(dnn::DNN{T})(x::AbstractVector{T}) where {T &lt;: Number}</code></pre><p>Let <code>N = DNN.ls[1].dims[2]</code> and <code>M = DNN.ls[end].dims[1]</code>, then here we treat the structure <code>DNN</code> as a function: <span>${\cal R}^N \mapsto {\cal R}^M$</span> Takes input <code>x</code> and passes it through each of the layers of <code>DNN</code>.</p><p><strong>Type Constraints</strong></p><ul><li><code>T &lt;: Number</code></li></ul><p><strong>Arguments</strong></p><ul><li><code>x :: T</code>  – An input value of dimension <code>N</code>.</li></ul><p><strong>Return</strong></p><p><code>::Vector{AD{T}}</code> of dimension <code>M</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/scottrsm/DNNS.jl/blob/06f29b15d3498bdac29fdcc614246b5f3ebbfb94/src/DNNS.jl#L118-L132">source</a></section></article><h2 id="Non-linear-Activation-Functions"><a class="docs-heading-anchor" href="#Non-linear-Activation-Functions">Non-linear Activation Functions</a><a id="Non-linear-Activation-Functions-1"></a><a class="docs-heading-anchor-permalink" href="#Non-linear-Activation-Functions" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DNNS.UtilFunc.sigmoid1" href="#DNNS.UtilFunc.sigmoid1"><code>DNNS.UtilFunc.sigmoid1</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">sigmoid1(x::AD{T})</code></pre><p>Implements an <code>AD</code> version of the standard &quot;exponential&quot; sigmoid function.</p><p><strong>Type Constraints</strong></p><ul><li>T &lt;: Number</li></ul><p><strong>Arguments</strong></p><ul><li>x   :: AD{T}  – The <code>AD</code> input value.</li></ul><p><strong>Return</strong></p><p>::AD{T} – The output AD value/derivative.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/scottrsm/DNNS.jl/blob/06f29b15d3498bdac29fdcc614246b5f3ebbfb94/src/UtilFunc.jl#L12-L25">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DNNS.UtilFunc.sigmoid2" href="#DNNS.UtilFunc.sigmoid2"><code>DNNS.UtilFunc.sigmoid2</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">sigmoid2(x::AD{T})</code></pre><p>Implements an <code>AD</code> version of the standard &quot;tanh&quot; sigmoid function.</p><p><strong>Type Constraints</strong></p><ul><li>T &lt;: Number</li></ul><p><strong>Arguments</strong></p><ul><li>x   :: AD{T}  – The <code>AD</code> input value.</li></ul><p><strong>Return</strong></p><p>::AD{T} – The output AD value/derivative.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/scottrsm/DNNS.jl/blob/06f29b15d3498bdac29fdcc614246b5f3ebbfb94/src/UtilFunc.jl#L35-L48">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DNNS.UtilFunc.sigmoid3" href="#DNNS.UtilFunc.sigmoid3"><code>DNNS.UtilFunc.sigmoid3</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">sigmoid3(x::AD{T})</code></pre><p>Implements an <code>AD</code> version of the standard &quot;arctan&quot; sigmoid function.</p><p><strong>Type Constraints</strong></p><ul><li>T &lt;: Number</li></ul><p><strong>Arguments</strong></p><ul><li>x   :: AD{T}  – The <code>AD</code> input value.</li></ul><p><strong>Return</strong></p><p>::AD{T} – The output AD value/derivative.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/scottrsm/DNNS.jl/blob/06f29b15d3498bdac29fdcc614246b5f3ebbfb94/src/UtilFunc.jl#L56-L69">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DNNS.UtilFunc.relu" href="#DNNS.UtilFunc.relu"><code>DNNS.UtilFunc.relu</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">relu(x::AD{T})</code></pre><p>Implements an <code>AD</code> version of the standard relu function.</p><p><strong>Type Constraints</strong></p><ul><li>T &lt;: Number</li></ul><p><strong>Arguments</strong></p><ul><li>x   :: AD{T}  – The <code>AD</code> input value.</li></ul><p><strong>Return</strong></p><p>::AD{T} – The output AD value/derivative.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/scottrsm/DNNS.jl/blob/06f29b15d3498bdac29fdcc614246b5f3ebbfb94/src/UtilFunc.jl#L78-L91">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DNNS.UtilFunc.relur" href="#DNNS.UtilFunc.relur"><code>DNNS.UtilFunc.relur</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">relur(x::AD{T})</code></pre><p>Implements an <code>AD</code> version of a modified version of the relu function. The modification is that while the value of the <code>relur</code> is the same as <code>relu</code>, its derivative is not. The value of the derivative is <code>0</code> or <code>1</code>, however the boundary moves randomly around the natural input boundary of <code>0</code>,</p><ul><li>T &lt;: Number</li></ul><p><strong>Arguments</strong></p><ul><li>x   :: AD{T}  – The <code>AD</code> input value.</li></ul><p><strong>Return</strong></p><p>::AD{T} – The output AD value/derivative.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/scottrsm/DNNS.jl/blob/06f29b15d3498bdac29fdcc614246b5f3ebbfb94/src/UtilFunc.jl#L98-L112">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DNNS.UtilFunc.softmax" href="#DNNS.UtilFunc.softmax"><code>DNNS.UtilFunc.softmax</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">softmax(x::Vector{T} [, τ=one(T)])</code></pre><p>Implements the <code>softmax</code> function.</p><p><strong>Type Constraints</strong></p><ul><li>T &lt;: Number</li></ul><p><strong>Arguments</strong></p><ul><li>x :: Vector{T}  – The <code>AD</code> input vector.</li><li>τ :: T          – The &quot;temperature&quot; parameter. </li></ul><p><strong>Return</strong></p><p>::Vector{T} – The output AD vector.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/scottrsm/DNNS.jl/blob/06f29b15d3498bdac29fdcc614246b5f3ebbfb94/src/UtilFunc.jl#L141-L155">source</a></section><section><div><pre><code class="language-julia hljs">softmax(x::Vector{AD{T}} [, τ=one(T)])</code></pre><p>Implements an <code>AD</code> version of the <code>softmax</code> function.</p><p><strong>Type Constraints</strong></p><ul><li>T &lt;: Number</li></ul><p><strong>Arguments</strong></p><ul><li>x :: Vector{AD{T}}  – The <code>AD</code> input vector.</li><li>τ :: T              – The &quot;temperature&quot; parameter. </li></ul><p><strong>Return</strong></p><p>::Vector{AD{T}} – The output AD vector.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/scottrsm/DNNS.jl/blob/06f29b15d3498bdac29fdcc614246b5f3ebbfb94/src/UtilFunc.jl#L167-L181">source</a></section></article><h2 id="Functions"><a class="docs-heading-anchor" href="#Functions">Functions</a><a id="Functions-1"></a><a class="docs-heading-anchor-permalink" href="#Functions" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DNNS.loss" href="#DNNS.loss"><code>DNNS.loss</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">loss(dnn, X, Y)</code></pre><p>Computes the loss of the neural network given inputs, <code>X</code>, and outputs <code>Y</code>.</p><p><strong>Type Constraints</strong></p><ul><li><code>T &lt;: Number</code></li></ul><p><strong>Arguments</strong></p><ul><li><code>dnn :: DNN{T}</code>     – A DNN layer.</li><li><code>X   :: Matrix{T}</code>  – The matrix of input values.</li><li><code>Y   :: Matrix{T}</code>  – The matrix of output values.</li></ul><p><strong>Return</strong></p><p><code>::AD{T}</code> – The loss of the network</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/scottrsm/DNNS.jl/blob/06f29b15d3498bdac29fdcc614246b5f3ebbfb94/src/DNNS.jl#L176-L191">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="DNNS.fit" href="#DNNS.fit"><code>DNNS.fit</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">fit(dnn, X, Y)</code></pre><p>Adjusts the parameters of neural network, <code>dnn</code>, to get the best fit of  the data: <code>X</code>, <code>Y</code>. The parameters of the network are all paris of  matrices and biases for each layer in the network.</p><p><strong>Type Constraints</strong></p><ul><li><code>T &lt;: Number</code></li></ul><p><strong>Arguments</strong></p><ul><li><code>dnn :: DNN{T}</code>     – A DNN layer.</li><li><code>X   :: Matrix{T}</code>  – The matrix of input values.</li><li><code>Y   :: Matrix{T}</code>  – The matrix of output values.</li></ul><p><strong>Return</strong></p><p><code>::nothing</code></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/scottrsm/DNNS.jl/blob/06f29b15d3498bdac29fdcc614246b5f3ebbfb94/src/DNNS.jl#L207-L224">source</a></section></article><h2 id="Index"><a class="docs-heading-anchor" href="#Index">Index</a><a id="Index-1"></a><a class="docs-heading-anchor-permalink" href="#Index" title="Permalink"></a></h2><ul><li><a href="#DNNS.AutoDiff.AD"><code>DNNS.AutoDiff.AD</code></a></li><li><a href="#DNNS.DLayer-Union{Tuple{DNNS.AutoDiff.AD{T}}, Tuple{T}} where T&lt;:Number"><code>DNNS.DLayer</code></a></li><li><a href="#DNNS.DLayer"><code>DNNS.DLayer</code></a></li><li><a href="#DNNS.DNN-Union{Tuple{AbstractVector{T}}, Tuple{T}} where T&lt;:Number"><code>DNNS.DNN</code></a></li><li><a href="#DNNS.DNN"><code>DNNS.DNN</code></a></li><li><a href="#DNNS.PWLF.PWL"><code>DNNS.PWLF.PWL</code></a></li><li><a href="#DNNS.PWLF.PWL-Tuple{T} where T&lt;:Number"><code>DNNS.PWLF.PWL</code></a></li><li><a href="#DNNS.UtilFunc.relu"><code>DNNS.UtilFunc.relu</code></a></li><li><a href="#DNNS.UtilFunc.relur"><code>DNNS.UtilFunc.relur</code></a></li><li><a href="#DNNS.UtilFunc.sigmoid1"><code>DNNS.UtilFunc.sigmoid1</code></a></li><li><a href="#DNNS.UtilFunc.sigmoid2"><code>DNNS.UtilFunc.sigmoid2</code></a></li><li><a href="#DNNS.UtilFunc.sigmoid3"><code>DNNS.UtilFunc.sigmoid3</code></a></li><li><a href="#DNNS.UtilFunc.softmax"><code>DNNS.UtilFunc.softmax</code></a></li><li><a href="#DNNS.fit"><code>DNNS.fit</code></a></li><li><a href="#DNNS.loss"><code>DNNS.loss</code></a></li></ul></article><nav class="docs-footer"><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.5.0 on <span class="colophon-date" title="Thursday 11 July 2024 12:34">Thursday 11 July 2024</span>. Using Julia version 1.10.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
